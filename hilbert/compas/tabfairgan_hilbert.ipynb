{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62512e8-2c23-49ed-931e-413f2809a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments(object):\n",
    "    def __init__(self, df_name, S, Y, underprivileged_value, desirable_value, num_epochs, batch_size, lambda_val, fake_name, size_of_fake_data):\n",
    "        self.df_name = df_name\n",
    "        self.S = S\n",
    "        self.Y = Y\n",
    "        self.underprivileged_value = underprivileged_value\n",
    "        self.desirable_value = desirable_value\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_val = lambda_val\n",
    "        self.fake_name = fake_name\n",
    "        self.size_of_fake_data = size_of_fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69114b26-7ece-4cd2-8cea-364cc2eb0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arguments(\"Compas_Raw_Scores_Modified.csv\", \"Ethnic_Code_Text\", \"binary_text\", \"African-American\", \"Low_Chance\", 100, 256, 1.0 , \"fake_compas.csv\", 16267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca579be-c852-419b-8538-1fec50ec0799",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"df_name\", help=\"Reference dataframe\", type=str)\n",
    "parser.add_argument(\"S\", help=\"Protected attribute\", type=str)\n",
    "parser.add_argument(\"Y\", help=\"Label (decision)\", type=str)\n",
    "parser.add_argument(\"underprivileged_value\", help=\"Value for underpriviledged group\", type=str)\n",
    "parser.add_argument(\"desirable_value\", help=\"Desired label (decision)\", type=str)\n",
    "\n",
    "parser.add_argument(\"num_epochs\", help=\"Total number of epochs\", type=int)\n",
    "parser.add_argument(\"batch_size\", help=\"the batch size\", type=int)\n",
    "#hilbert change\n",
    "#parser.add_argument(\"num_fair_epochs\", help=\"number of fair training epochs\", type=int)\n",
    "parser.add_argument(\"lambda_val\", help=\"lambda parameter\", type=float)\n",
    "parser.add_argument(\"fake_name\", help=\"name of the produced csv file\", type=str)\n",
    "parser.add_argument(\"size_of_fake_data\", help=\"how many data records to generate\", type=int)\n",
    "args = parser.parse_args()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "S = args.S\n",
    "Y = args.Y\n",
    "S_under = args.underprivileged_value\n",
    "Y_desire = args.desirable_value\n",
    "\n",
    "df = pd.read_csv(args.df_name)\n",
    "\n",
    "df[S] = df[S].astype(object)\n",
    "df[Y] = df[Y].astype(object)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ohe_data(df):\n",
    "    df_int = df.select_dtypes(['float', 'integer']).values\n",
    "    continuous_columns_list = list(df.select_dtypes(['float', 'integer']).columns)\n",
    "    ##############################################################\n",
    "    scaler = QuantileTransformer(n_quantiles=2000, output_distribution='uniform')\n",
    "    df_int = scaler.fit_transform(df_int)\n",
    "\n",
    "    df_cat = df.select_dtypes('object')\n",
    "    df_cat_names = list(df.select_dtypes('object').columns)\n",
    "    numerical_array = df_int\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_array = ohe.fit_transform(df_cat)\n",
    "\n",
    "    cat_lens = [i.shape[0] for i in ohe.categories_]\n",
    "    discrete_columns_ordereddict = OrderedDict(zip(df_cat_names, cat_lens))\n",
    "\n",
    "    S_start_index = len(continuous_columns_list) + sum(\n",
    "        list(discrete_columns_ordereddict.values())[:list(discrete_columns_ordereddict.keys()).index(S)])\n",
    "    Y_start_index = len(continuous_columns_list) + sum(\n",
    "        list(discrete_columns_ordereddict.values())[:list(discrete_columns_ordereddict.keys()).index(Y)])\n",
    "\n",
    "    if ohe.categories_[list(discrete_columns_ordereddict.keys()).index(S)][0] == S_under:\n",
    "        underpriv_index = 0\n",
    "        priv_index = 1\n",
    "    else:\n",
    "        underpriv_index = 1\n",
    "        priv_index = 0\n",
    "    if ohe.categories_[list(discrete_columns_ordereddict.keys()).index(Y)][0] == Y_desire:\n",
    "        desire_index = 0\n",
    "        undesire_index = 1\n",
    "    else:\n",
    "        desire_index = 1\n",
    "        undesire_index = 0\n",
    "\n",
    "    final_array = np.hstack((numerical_array, ohe_array.toarray()))\n",
    "    return ohe, scaler, discrete_columns_ordereddict, continuous_columns_list, final_array, S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index\n",
    "\n",
    "\n",
    "def get_original_data(df_transformed, df_orig, ohe, scaler):\n",
    "    # df_int = df_orig.select_dtypes(['float','integer'])\n",
    "    df_ohe_int = df_transformed[:, :df_orig.select_dtypes(['float', 'integer']).shape[1]]\n",
    "    df_ohe_int = scaler.inverse_transform(df_ohe_int)\n",
    "    df_ohe_cats = df_transformed[:, df_orig.select_dtypes(['float', 'integer']).shape[1]:]\n",
    "    df_ohe_cats = ohe.inverse_transform(df_ohe_cats)\n",
    "    # df_income = df_transformed[:,-1]\n",
    "    # df_ohe_cats = np.hstack((df_ohe_cats, df_income.reshape(-1,1)))\n",
    "    df_int = pd.DataFrame(df_ohe_int, columns=df_orig.select_dtypes(['float', 'integer']).columns)\n",
    "    df_cat = pd.DataFrame(df_ohe_cats, columns=df_orig.select_dtypes('object').columns)\n",
    "    return pd.concat([df_int, df_cat], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(df, batch_size):\n",
    "    #df = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "    ohe, scaler, discrete_columns, continuous_columns, df_transformed, S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index = get_ohe_data(df)\n",
    "\n",
    "\n",
    "    input_dim = df_transformed.shape[1]\n",
    "\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #################\n",
    "    X_train, X_test = train_test_split(df_transformed,test_size=0.1, shuffle=True) #random_state=10)\n",
    "    #X_train = df_transformed[:df_train.shape[0],:]\n",
    "    #X_test = df_transformed[df_train.shape[0]:,:]\n",
    "\n",
    "    data_train = X_train.copy()\n",
    "    data_test = X_test.copy()\n",
    "\n",
    "    from torch.utils.data import TensorDataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    data = torch.from_numpy(data_train).float()\n",
    "\n",
    "\n",
    "    train_ds = TensorDataset(data)\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size, drop_last=True)\n",
    "    return ohe, scaler, input_dim, discrete_columns, continuous_columns ,train_dl, data_train, data_test, S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, continuous_columns, discrete_columns):\n",
    "        super(Generator, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._discrete_columns = discrete_columns\n",
    "        self._num_continuous_columns = len(continuous_columns)\n",
    "\n",
    "        self.lin1 = nn.Linear(self._input_dim, self._input_dim)\n",
    "        self.lin_numerical = nn.Linear(self._input_dim, self._num_continuous_columns)\n",
    "\n",
    "        self.lin_cat = nn.ModuleDict()\n",
    "        for key, value in self._discrete_columns.items():\n",
    "            self.lin_cat[key] = nn.Linear(self._input_dim, value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        # x = f.leaky_relu(self.lin1(x))\n",
    "        # x_numerical = f.leaky_relu(self.lin_numerical(x))\n",
    "        x_numerical = f.relu(self.lin_numerical(x))\n",
    "        x_cat = []\n",
    "        for key in self.lin_cat:\n",
    "            x_cat.append(f.gumbel_softmax(self.lin_cat[key](x), tau=0.2))\n",
    "        x_final = torch.cat((x_numerical, *x_cat), 1)\n",
    "        return x_final\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        # self.dense1 = nn.Linear(109, 256)\n",
    "        self.dense1 = nn.Linear(self._input_dim, self._input_dim)\n",
    "        self.dense2 = nn.Linear(self._input_dim, self._input_dim)\n",
    "        # self.dense3 = nn.Linear(256, 1)\n",
    "        # self.drop = nn.Dropout(p=0.2)\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.leaky_relu(self.dense1(x))\n",
    "        # x = self.drop(x)\n",
    "        # x = f.leaky_relu(self.dense2(x))\n",
    "        x = f.leaky_relu(self.dense2(x))\n",
    "        # x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def pairwise_distances(x):\n",
    "    #x should be two dimensional\n",
    "    instances_norm = torch.sum(x**2,-1).reshape((-1,1))\n",
    "    return -2*torch.mm(x,x.t()) + instances_norm + instances_norm.t()\n",
    "\n",
    "def GaussianKernelMatrix(x, sigma=1):\n",
    "    if sigma == 0:\n",
    "        sigma = 0.0001\n",
    "    pairwise_distances_ = pairwise_distances(x)\n",
    "    return torch.exp(-pairwise_distances_ /sigma)\n",
    "\n",
    "def HSIC(X, Y):\n",
    "    n = X.shape[0]\n",
    "    Xmed = X\n",
    "    G = torch.sum(Xmed*Xmed, 1).reshape(n,1)\n",
    "    Q = torch.tile(G, (1,n))\n",
    "    R = torch.tile(G.T, (n,1))\n",
    "    \n",
    "    dists = Q + R - 2*torch.matmul(Xmed, Xmed.T)\n",
    "    dists = dists - torch.tril(dists)\n",
    "    dists = dists.reshape(n**2, 1)\n",
    "    \n",
    "    if not dists[dists>0].nelement() == 0:\n",
    "        s_x = torch.sqrt( 0.5 * torch.median(dists[dists>0]) )\n",
    "    else:\n",
    "        s_x = 0\n",
    "\n",
    "    n = Y.shape[0]\n",
    "    Ymed = Y\n",
    "    G = torch.sum(Ymed*Ymed, 1).reshape(n,1)\n",
    "    Q = torch.tile(G, (1,n))\n",
    "    R = torch.tile(G.T, (n,1))\n",
    "    \n",
    "    dists = Q + R - 2*torch.matmul(Ymed, Ymed.T)\n",
    "    dists = dists - torch.tril(dists)\n",
    "    dists = dists.reshape(n**2, 1)\n",
    "    \n",
    "    if not dists[dists>0].nelement() == 0:\n",
    "        s_y = torch.sqrt( 0.5 * torch.median(dists[dists>0]) )\n",
    "    else:\n",
    "        s_y = 0\n",
    "    \n",
    "    m,_ = X.shape #batch size\n",
    "    K = GaussianKernelMatrix(X,s_x)\n",
    "    L = GaussianKernelMatrix(Y,s_y)\n",
    "    H = torch.eye(m) - 1.0/m * torch.ones((m,m))\n",
    "    H = H.float().cuda()\n",
    "    HSIC = torch.trace(torch.mm(L,torch.mm(H,torch.mm(K,H))))/((m-1)**2)\n",
    "    return HSIC\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "class FairLossFunc(nn.Module):\n",
    "    def __init__(self, S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index):\n",
    "        super(FairLossFunc, self).__init__()\n",
    "        self._S_start_index = S_start_index\n",
    "        self._Y_start_index = Y_start_index\n",
    "        self._underpriv_index = underpriv_index\n",
    "        self._priv_index = priv_index\n",
    "        self._undesire_index = undesire_index\n",
    "        self._desire_index = desire_index\n",
    "\n",
    "    def forward(self, x, crit_fake_pred, lamda):\n",
    "        #G = x[:, self._S_start_index:self._S_start_index + 2]\n",
    "        G = x[:, self._S_start_index:self._S_start_index + 1]\n",
    "\n",
    "        # print(x[0,64])\n",
    "        #I = x[:, self._Y_start_index:self._Y_start_index + 2]\n",
    "        I = x[:, self._Y_start_index:self._Y_start_index + 1]\n",
    "        #print(G.shape)\n",
    "        #print(I.shape)\n",
    "        # disp = (torch.mean(G[:,1]*I[:,1])/(x[:,65].sum())) - (torch.mean(G[:,0]*I[:,0])/(x[:,64].sum()))\n",
    "        # disp = -1.0 * torch.tanh(torch.mean(G[:,0]*I[:,1])/(x[:,64].sum()) - torch.mean(G[:,1]*I[:,1])/(x[:,65].sum()))\n",
    "        # gen_loss = -1.0 * torch.mean(crit_fake_pred)\n",
    "        \n",
    "        \"\"\"\n",
    "        disp = -1.0 * lamda * (torch.mean(G[:, self._underpriv_index] * I[:, self._desire_index]) / (\n",
    "            x[:, self._S_start_index + self._underpriv_index].sum()) - torch.mean(\n",
    "            G[:, self._priv_index] * I[:, self._desire_index]) / (\n",
    "                                   x[:, self._S_start_index + self._priv_index].sum())) - 1.0 * torch.mean(\n",
    "            crit_fake_pred)\n",
    "        \n",
    "        \"\"\"\n",
    "        disp = 1.0 * lamda * HSIC(G,I) - 1.0 * torch.mean(crit_fake_pred)\n",
    "        \n",
    "        \n",
    "        # print(disp)\n",
    "        return disp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "\n",
    "\n",
    "def get_gradient(crit, real, fake, epsilon):\n",
    "    mixed_data = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "    mixed_scores = crit(mixed_data)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=mixed_data,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def gradient_penalty(gradient):\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def get_gen_loss(crit_fake_pred):\n",
    "    gen_loss = -1. * torch.mean(crit_fake_pred)\n",
    "\n",
    "    return gen_loss\n",
    "\n",
    "\n",
    "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
    "    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp\n",
    "\n",
    "    return crit_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display_step = 50\n",
    "\n",
    "\n",
    "def train(df, epochs=500, batch_size=64, lamda=0.5):\n",
    "    ohe, scaler, input_dim, discrete_columns, continuous_columns, train_dl, data_train, data_test, S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index = prepare_data(\n",
    "        df, batch_size)\n",
    "\n",
    "    generator = Generator(input_dim, continuous_columns, discrete_columns).to(device)\n",
    "    critic = Critic(input_dim).to(device)\n",
    "    second_critic = FairLossFunc(S_start_index, Y_start_index, underpriv_index, priv_index, undesire_index, desire_index).to(\n",
    "        device)\n",
    "\n",
    "    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    gen_optimizer_fair = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    crit_optimizer = torch.optim.Adam(critic.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # loss = nn.BCELoss()\n",
    "    critic_losses = []\n",
    "    generator_losses = []\n",
    "    cur_step = 0\n",
    "    for i in range(epochs):\n",
    "        # j = 0\n",
    "        #print(\"epoch {}\".format(i + 1))\n",
    "        ############################\n",
    "        #hilbert change\n",
    "        #if i + 1 <= (epochs - fair_epochs):\n",
    "        #    print(\"training for accuracy\")\n",
    "        #if i + 1 > (epochs - fair_epochs):\n",
    "        #    print(\"training for fairness\")\n",
    "        for data in train_dl:\n",
    "            data[0] = data[0].to(device)\n",
    "            # j += 1\n",
    "            loss_of_epoch_G = 0\n",
    "            loss_of_epoch_D = 0\n",
    "            crit_repeat = 5\n",
    "            mean_iteration_critic_loss = 0\n",
    "            for k in range(crit_repeat):\n",
    "                # training the critic\n",
    "                crit_optimizer.zero_grad()\n",
    "                fake_noise = torch.randn(size=(batch_size, input_dim), device=device).float()\n",
    "                fake = generator(fake_noise)\n",
    "\n",
    "                crit_fake_pred = critic(fake.detach())\n",
    "                crit_real_pred = critic(data[0])\n",
    "\n",
    "                epsilon = torch.rand(batch_size, input_dim, device=device, requires_grad=True)\n",
    "                gradient = get_gradient(critic, data[0], fake.detach(), epsilon)\n",
    "                gp = gradient_penalty(gradient)\n",
    "\n",
    "                crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda=10)\n",
    "\n",
    "                mean_iteration_critic_loss += crit_loss.item() / crit_repeat\n",
    "                crit_loss.backward(retain_graph=True)\n",
    "                crit_optimizer.step()\n",
    "            #############################\n",
    "            if cur_step > 50:\n",
    "                critic_losses += [mean_iteration_critic_loss]\n",
    "\n",
    "            #############################\n",
    "            #hilbert change\n",
    "            # training the generator for fairness\n",
    "            # training the generator for accuracy\n",
    "\n",
    "\n",
    "            gen_optimizer_fair.zero_grad()\n",
    "            fake_noise_2 = torch.randn(size=(batch_size, input_dim), device=device).float()\n",
    "            fake_2 = generator(fake_noise_2)\n",
    "            crit_fake_pred_2 = critic(fake_2)\n",
    "            gen_fair_loss = second_critic(fake_2, crit_fake_pred_2, lamda)\n",
    "            gen_fair_loss.backward()\n",
    "            gen_optimizer_fair.step()\n",
    "            \n",
    "            #hilbert change\n",
    "            \"\"\"\n",
    "            if i + 1 <= (epochs - fair_epochs):\n",
    "                # training the generator for accuracy\n",
    "                gen_optimizer.zero_grad()\n",
    "                fake_noise_2 = torch.randn(size=(batch_size, input_dim), device=device).float()\n",
    "                fake_2 = generator(fake_noise_2)\n",
    "                crit_fake_pred = critic(fake_2)\n",
    "\n",
    "                gen_loss = get_gen_loss(crit_fake_pred)\n",
    "                gen_loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                gen_optimizer.step()\n",
    "\n",
    "            ###############################\n",
    "            if i + 1 > (epochs - fair_epochs):\n",
    "                # training the generator for fairness\n",
    "                gen_optimizer_fair.zero_grad()\n",
    "                fake_noise_2 = torch.randn(size=(batch_size, input_dim), device=device).float()\n",
    "                fake_2 = generator(fake_noise_2)\n",
    "                crit_fake_pred = critic(fake_2)\n",
    "\n",
    "                gen_fair_loss = second_critic(fake_2, crit_fake_pred, lamda)\n",
    "                gen_fair_loss.backward()\n",
    "                gen_optimizer_fair.step()\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            # Keep track of the average generator loss\n",
    "            #################################\n",
    "            if cur_step > 50:\n",
    "                \"\"\"\n",
    "                if i + 1 <= (epochs - fair_epochs):\n",
    "                    generator_losses += [gen_loss.item()]\n",
    "                if i + 1 > (epochs - fair_epochs):\n",
    "                    generator_losses += [gen_fair_loss.item()]\n",
    "                \"\"\"\n",
    "                generator_losses += [gen_fair_loss.item()]\n",
    "\n",
    "                \n",
    "            \"\"\"\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "                crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
    "                print(\"Step {}: Generator loss: {}, critic loss: {}\".format(cur_step, gen_mean, crit_mean))\n",
    "                step_bins = 20\n",
    "                num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "                #plt.ion()\n",
    "                plt.plot(\n",
    "                    range(num_examples // step_bins),\n",
    "                    torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                    label=\"Generator Loss\"\n",
    "                )\n",
    "                plt.plot(\n",
    "                    range(num_examples // step_bins),\n",
    "                    torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                    label=\"Critic Loss\"\n",
    "                )\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            \"\"\"\n",
    "                  # print(\"cr step: {}\".format(cur_step))\n",
    "            \n",
    "            \n",
    "                    \n",
    "            cur_step += 1\n",
    "\n",
    "    return generator, critic, ohe, scaler, data_train, data_test, input_dim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#generator, critic, ohe, scaler, data_train, data_test = train_plot(df, args.num_epochs,args.batch_size, args.num_fair_epochs, args.lambda_val)\n",
    "def train_plot(df, epochs, batchsize, lamda):\n",
    "    generator, critic, ohe, scaler, data_train, data_test, input_dim = train(df, epochs, batchsize, lamda)\n",
    "    return generator, critic, ohe, scaler, data_train, data_test, input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "005c4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"adult.csv\")\n",
    "#generator, critic, ohe, scaler, data_train, data_test, input_dim = train_plot(df, 100, args.batch_size, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bc1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b35beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, lambda: 1.0\n",
      "epoch: 100, lambda: 1.2\n",
      "epoch: 100, lambda: 1.4\n",
      "epoch: 100, lambda: 1.6\n",
      "epoch: 100, lambda: 1.8\n",
      "epoch: 100, lambda: 2\n",
      "epoch: 150, lambda: 1.0\n",
      "epoch: 150, lambda: 1.2\n",
      "epoch: 150, lambda: 1.4\n",
      "epoch: 150, lambda: 1.6\n",
      "epoch: 150, lambda: 1.8\n",
      "epoch: 150, lambda: 2\n"
     ]
    }
   ],
   "source": [
    "outfile = \"results.log\"\n",
    "\n",
    "def print2file(buf, outFile):\n",
    "    outfd = open(outFile, 'a')\n",
    "    outfd.write(buf + '\\n')\n",
    "    outfd.close()\n",
    "\n",
    "firstline = \"epoch, lambda, acc, f1, pmale, pfemale, dp, omale, ofemale, deo\"\n",
    "print2file(firstline, outfile)\n",
    "#epoch_val = 100\n",
    "for epoch_val in [100,150]:\n",
    "#for epoch_val in np.linspace(1,5,5):\n",
    "    #for lambda_val in np.linspace(0.05,0.15,11):\n",
    "    for lambda_val in [1.0,1.2,1.4,1.6,1.8,2]:\n",
    "        #for repetition in range(5):\n",
    "        \n",
    "        print(\"epoch: {}, lambda: {}\".format(epoch_val, lambda_val))\n",
    "        #print(\"epochs: {}, lambda: {}\".format(int(epoch_val), lambda_val))\n",
    "        #generator, critic, ohe, scaler, data_train, data_test, input_dim = train_plot(df, args.num_epochs, args.batch_size, args.num_fair_epochs, args.lambda_val)\n",
    "        ##hilbert change\n",
    "        df = pd.read_csv(\"Compas_Raw_Scores_Modified.csv\")\n",
    "        generator, critic, ohe, scaler, data_train, data_test, input_dim = train_plot(df, int(epoch_val), args.batch_size, lambda_val)\n",
    "        \n",
    "        df = pd.read_csv(\"Compas_Raw_Scores_Modified.csv\")\n",
    "        with torch.no_grad():\n",
    "            fake_numpy_array = generator(torch.randn(size=(args.size_of_fake_data, input_dim), device=device)).cpu().detach().numpy()\n",
    "\n",
    "        fake_df = get_original_data(fake_numpy_array, df, ohe, scaler)\n",
    "        fake_df = fake_df[df.columns]\n",
    "        fake_df.to_csv(args.fake_name, index=False)\n",
    "\n",
    "        df_fake = pd.read_csv(\"fake_compas.csv\")\n",
    "        df_fake = pd.get_dummies(df_fake)\n",
    "\n",
    "        df = pd.get_dummies(df)\n",
    "\n",
    "        for i in df.columns:\n",
    "            if df[i].dtype == \"int64\":\n",
    "                df[i] = df[i].astype(float)\n",
    "\n",
    "        for i in df.columns:\n",
    "            if i not in df_fake:\n",
    "                df_fake[i] = 0\n",
    "\n",
    "        df_fake = df_fake[df.columns]\n",
    "\n",
    "\n",
    "        df = df[df.columns.drop('Ethnic_Code_Text_African-American')]\n",
    "        df = df[df.columns.drop('binary_text_High_Chance')]\n",
    "\n",
    "        df_fake = df_fake[df_fake.columns.drop('Ethnic_Code_Text_African-American')]\n",
    "        df_fake = df_fake[df_fake.columns.drop('binary_text_High_Chance')]\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        a, df_test = train_test_split(df, test_size=0.2)\n",
    "        df_train = df_fake\n",
    "        df_train_x = df_train.loc[:,df.columns != 'binary_text_Low_Chance']\n",
    "        df_train_y = df_train['binary_text_Low_Chance']\n",
    "        df_test_x = df_test.loc[:,df.columns != 'binary_text_Low_Chance']\n",
    "        df_test_y = df_test['binary_text_Low_Chance']\n",
    "\n",
    "        #from sklearn.neural_network import MLPClassifier\n",
    "        #clf = MLPClassifier()\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        clf = DecisionTreeClassifier()\n",
    "\n",
    "        clf = clf.fit(df_train_x, df_train_y)\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        #print(\"accuracy  = {}\".format(accuracy_score(clf.predict(df_test_x), df_test_y)))\n",
    "        theaccuracy = accuracy_score(clf.predict(df_test_x), df_test_y)\n",
    "        \n",
    "        from sklearn.metrics import f1_score\n",
    "        thef1 = f1_score(clf.predict(df_test_x), df_test_y)\n",
    "        #print(\"f1  = {}\".format(f1_score(clf.predict(df_test_x), df_test_y)))\n",
    "\n",
    "\n",
    "        df_test_x['pred'] = clf.predict(df_test_x)\n",
    "        df_test_x['real'] = df_test_y\n",
    "        pmale = df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 1) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['Ethnic_Code_Text_Caucasian'] == 1].shape[0]\n",
    "        pfemale = df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 0) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['Ethnic_Code_Text_Caucasian'] == 0].shape[0]\n",
    "        DP = pmale - pfemale\n",
    "\n",
    "        omale = df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 1) & (df_test_x['pred'] == 1) & (df_test_x['real'] == 1)].shape[0] / df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 1) & (df_test_x['real'] == 1)].shape[0]\n",
    "        ofemale = df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 0) & (df_test_x['pred'] == 1) & (df_test_x['real'] == 1)].shape[0] / df_test_x[(df_test_x['Ethnic_Code_Text_Caucasian'] == 0) & (df_test_x['real'] == 1)].shape[0]\n",
    "        DiffEO = omale - ofemale\n",
    "\n",
    "\n",
    "        the_line = \"%d, %f, %f, %f, %f, %f, %f, %f, %f, %f\" % (epoch_val, lambda_val, theaccuracy, thef1, pmale, pfemale, DP, omale, ofemale, DiffEO)\n",
    "        print2file(the_line, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4464d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ccbedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, critic, ohe, scaler, data_train, data_test, input_dim = train_plot(df, 100, args.batch_size, 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7a47af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy original = 0.8204708290685773\n",
      "f1_original = 0.6279168434450573\n",
      "accuracy generated = 0.7545547594677584\n",
      "f1_generated = 0.5428898208158597\n"
     ]
    }
   ],
   "source": [
    "data_train_x = data_train[:, :-2]\n",
    "data_train_y = np.argmax(data_train[:, -2:], axis=1)\n",
    "\n",
    "data_test_x = data_test[:, :-2]\n",
    "data_test_y = np.argmax(data_test[:, -2:], axis=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    df_generated = generator(torch.randn(size=(32561, input_dim), device=device)).cpu().detach().numpy()\n",
    "\n",
    "df_generated_x = df_generated[:, :-2]\n",
    "df_generated_y = np.argmax(df_generated[:, -2:], axis=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(data_train_x, data_train_y)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy original = {}\".format(accuracy_score(clf.predict(data_test_x), data_test_y)))\n",
    "#accuracy_original = accuracy_score(clf.predict(data_test_x), data_test_y)\n",
    "from sklearn.metrics import f1_score\n",
    "#f1_original = f1_score(clf.predict(data_test_x), data_test_y)\n",
    "print(\"f1_original = {}\".format(f1_score(clf.predict(data_test_x), data_test_y)))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(df_generated_x, df_generated_y)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy generated = {}\".format(accuracy_score(clf.predict(data_test_x), data_test_y)))\n",
    "#accuracy_generated = accuracy_score(clf.predict(data_test_x), data_test_y)\n",
    "from sklearn.metrics import f1_score\n",
    "#f1_generated = f1_score(clf.predict(data_test_x), data_test_y)\n",
    "print(\"f1_generated = {}\".format(f1_score(clf.predict(data_test_x), data_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "be918a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\")\n",
    "with torch.no_grad():\n",
    "    fake_numpy_array = generator(torch.randn(size=(args.size_of_fake_data, input_dim), device=device)).cpu().detach().numpy()\n",
    "\n",
    "fake_df = get_original_data(fake_numpy_array, df, ohe, scaler)\n",
    "fake_df = fake_df[df.columns]\n",
    "fake_df.to_csv(args.fake_name, index=False)\n",
    "\n",
    "df_fake = pd.read_csv(\"fake_adult.csv\")\n",
    "df_fake = pd.get_dummies(df_fake)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == \"int64\":\n",
    "        df[i] = df[i].astype(float)\n",
    "\n",
    "for i in df.columns:\n",
    "    if i not in df_fake:\n",
    "        df_fake[i] = 0\n",
    "\n",
    "df_fake = df_fake[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5e041ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  = 0.757702937864674\n",
      "f1  = 0.5417231364956439\n"
     ]
    }
   ],
   "source": [
    "df = df[df.columns.drop('sex_ Female')]\n",
    "df = df[df.columns.drop('income_ <=50K')]\n",
    "\n",
    "df_fake = df_fake[df_fake.columns.drop('sex_ Female')]\n",
    "df_fake = df_fake[df_fake.columns.drop('income_ <=50K')]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "a, df_test = train_test_split(df, test_size=0.2)\n",
    "df_train = df_fake\n",
    "df_train_x = df_train.loc[:,df.columns != 'income_ >50K']\n",
    "df_train_y = df_train['income_ >50K']\n",
    "df_test_x = df_test.loc[:,df.columns != 'income_ >50K']\n",
    "df_test_y = df_test['income_ >50K']\n",
    "\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#clf = MLPClassifier()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf = clf.fit(df_train_x, df_train_y)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy  = {}\".format(accuracy_score(clf.predict(df_test_x), df_test_y)))\n",
    "#accuracy_original = accuracy_score(clf.predict(data_test_x), data_test_y)\n",
    "from sklearn.metrics import f1_score\n",
    "#f1_original = f1_score(clf.predict(data_test_x), data_test_y)\n",
    "print(\"f1  = {}\".format(f1_score(clf.predict(df_test_x), df_test_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d55e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "05048a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.299079754601227\n",
      "0.2486919052016005\n"
     ]
    }
   ],
   "source": [
    "df_test_x['pred'] = clf.predict(df_test_x)\n",
    "df_test_x['real'] = df_test_y\n",
    "print(df_test_x[(df_test_x['sex_ Male'] == 1) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['sex_ Male'] == 1].shape[0])\n",
    "print(df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['sex_ Male'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "072c0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.299079754601227\n"
     ]
    }
   ],
   "source": [
    "print(df_test_x[(df_test_x['sex_ Male'] == 1) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['sex_ Male'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "12e6af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2486919052016005\n"
     ]
    }
   ],
   "source": [
    "print(df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['pred'] == 1)].shape[0] / df_test_x[df_test_x['sex_ Male'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "762a611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##28 18\n",
    "##55 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a829d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "b6f39b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5632126921170054\n",
      "0.6743589743589744\n"
     ]
    }
   ],
   "source": [
    "print(df_test_x[(df_test_x['sex_ Male'] == 1) & (df_test_x['pred'] == 1) & (df_test_x['real'] == 1)].shape[0] / df_test_x[(df_test_x['sex_ Male'] == 1) & (df_test_x['real'] == 1)].shape[0])\n",
    "print(df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['pred'] == 1) & (df_test_x['real'] == 1)].shape[0] / df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['real'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "931e5414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578171091445427"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['pred'] == 1) & (df_test_x['real'] == 1)].shape[0] / df_test_x[(df_test_x['sex_ Male'] == 0) & (df_test_x['real'] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cc086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f75d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7ac4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_numpy_array = generator(torch.randn(size=(args.size_of_fake_data, input_dim), device=device)).cpu().detach().numpy()\n",
    "fake_df = get_original_data(fake_numpy_array, df, ohe, scaler)\n",
    "fake_df = fake_df[df.columns]\n",
    "fake_df.to_csv(args.fake_name, index=False)\n",
    "####finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d4b2e27-43ef-46d8-9bb2-0f01f5fc9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dc6f0b4-0c04-45ef-b2ff-60989ac67100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv(\"fake_adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97569f34-a82c-4170-bc70-03a4e07d4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d490bb-5326-4941-bcce-80baaf21d0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "table_evaluator = TableEvaluator(df, fake)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958d6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b86fbeebefdb973cc4ec1488ea9559e065416a03860fff0abc90996369f83dde"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
